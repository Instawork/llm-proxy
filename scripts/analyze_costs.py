#!/usr/bin/env python3
"""
Cost Analysis Script for LLM Proxy

This script analyzes the cost tracking data generated by the LLM proxy
and provides various useful reports.

Usage:
    python scripts/analyze_costs.py [cost_file] [--since HOURS] [--format FORMAT]

Examples:
    # Analyze last 24 hours with summary
    python scripts/analyze_costs.py logs/cost-tracking.jsonl

    # Analyze last 7 days
    python scripts/analyze_costs.py logs/cost-tracking.jsonl --since 168

    # Generate CSV report
    python scripts/analyze_costs.py logs/cost-tracking.jsonl --format csv
"""

import json
import argparse
import sys
from datetime import datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Any
import csv


def load_cost_records(filename: str, since_hours: int = 24) -> List[Dict[str, Any]]:
    """Load cost records from JSONL file, filtering by time."""
    cutoff_time = datetime.now() - timedelta(hours=since_hours)
    records = []

    try:
        with open(filename, "r") as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    # Parse timestamp
                    timestamp = datetime.fromisoformat(
                        record["timestamp"].replace("Z", "+00:00")
                    )
                    if timestamp >= cutoff_time:
                        records.append(record)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    print(f"Warning: Skipping invalid record: {e}", file=sys.stderr)
                    continue
    except FileNotFoundError:
        print(f"Error: Cost tracking file '{filename}' not found.", file=sys.stderr)
        print(
            "Make sure the LLM proxy has been running and processing requests.",
            file=sys.stderr,
        )
        return []

    return sorted(records, key=lambda x: x["timestamp"])


def generate_summary_report(records: List[Dict[str, Any]]) -> None:
    """Generate a summary report of costs."""
    if not records:
        print("No records found for the specified time period.")
        return

    total_cost = sum(r["total_cost"] for r in records)
    total_requests = len(records)
    total_tokens = sum(r["total_tokens"] for r in records)

    # Group by provider
    provider_stats = defaultdict(lambda: {"cost": 0.0, "requests": 0, "tokens": 0})
    model_stats = defaultdict(lambda: {"cost": 0.0, "requests": 0, "tokens": 0})

    for record in records:
        provider = record["provider"]
        model = f"{record['provider']}/{record['model']}"

        provider_stats[provider]["cost"] += record["total_cost"]
        provider_stats[provider]["requests"] += 1
        provider_stats[provider]["tokens"] += record["total_tokens"]

        model_stats[model]["cost"] += record["total_cost"]
        model_stats[model]["requests"] += 1
        model_stats[model]["tokens"] += record["total_tokens"]

    # Print summary
    print("=" * 60)
    print("LLM PROXY COST ANALYSIS SUMMARY")
    print("=" * 60)
    print(f"Time Period: {records[0]['timestamp']} to {records[-1]['timestamp']}")
    print(f"Total Cost: ${total_cost:.6f}")
    print(f"Total Requests: {total_requests:,}")
    print(f"Total Tokens: {total_tokens:,}")
    print(
        f"Average Cost per Request: ${total_cost / total_requests:.6f}"
        if total_requests > 0
        else "N/A"
    )
    print(
        f"Average Cost per 1K Tokens: ${(total_cost * 1000) / total_tokens:.6f}"
        if total_tokens > 0
        else "N/A"
    )
    print()

    # Provider breakdown
    print("COST BY PROVIDER:")
    print("-" * 40)
    for provider, stats in sorted(
        provider_stats.items(), key=lambda x: x[1]["cost"], reverse=True
    ):
        print(
            f"{provider:12} | ${stats['cost']:>8.6f} | {stats['requests']:>6,} req | {stats['tokens']:>8,} tokens"
        )
    print()

    # Top models
    print("TOP MODELS BY COST:")
    print("-" * 50)
    sorted_models = sorted(
        model_stats.items(), key=lambda x: x[1]["cost"], reverse=True
    )[:10]
    for model, stats in sorted_models:
        print(
            f"{model:25} | ${stats['cost']:>8.6f} | {stats['requests']:>6,} req | {stats['tokens']:>8,} tokens"
        )
    print()


def generate_detailed_report(records: List[Dict[str, Any]]) -> None:
    """Generate a detailed report showing individual requests."""
    print("DETAILED REQUEST LOG:")
    print("-" * 100)
    print(
        f"{'Timestamp':19} | {'Provider/Model':25} | {'Tokens':>8} | {'Cost':>10} | {'User':15}"
    )
    print("-" * 100)

    for record in records:
        timestamp = record["timestamp"][:19]  # Trim to YYYY-MM-DD HH:MM:SS
        model = f"{record['provider']}/{record['model']}"[:25]
        user = record.get("user_id", "unknown")[:15]

        print(
            f"{timestamp} | {model:25} | {record['total_tokens']:>8,} | ${record['total_cost']:>9.6f} | {user:15}"
        )


def generate_csv_report(records: List[Dict[str, Any]], filename: str = None) -> None:
    """Generate a CSV report."""
    if filename is None:
        filename = f"cost_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

    fieldnames = [
        "timestamp",
        "provider",
        "model",
        "user_id",
        "endpoint",
        "is_streaming",
        "input_tokens",
        "output_tokens",
        "total_tokens",
        "input_cost",
        "output_cost",
        "total_cost",
    ]

    with open(filename, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for record in records:
            row = {field: record.get(field, "") for field in fieldnames}
            writer.writerow(row)

    print(f"CSV report generated: {filename}")


def generate_hourly_breakdown(records: List[Dict[str, Any]]) -> None:
    """Generate hourly cost breakdown."""
    hourly_stats = defaultdict(lambda: {"cost": 0.0, "requests": 0})

    for record in records:
        # Extract hour from timestamp
        timestamp = datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00"))
        hour_key = timestamp.strftime("%Y-%m-%d %H:00")

        hourly_stats[hour_key]["cost"] += record["total_cost"]
        hourly_stats[hour_key]["requests"] += 1

    print("HOURLY BREAKDOWN:")
    print("-" * 40)
    print(f"{'Hour':16} | {'Cost':>10} | {'Requests':>9}")
    print("-" * 40)

    for hour, stats in sorted(hourly_stats.items()):
        print(f"{hour:16} | ${stats['cost']:>9.6f} | {stats['requests']:>9,}")
    print()


def main():
    parser = argparse.ArgumentParser(description="Analyze LLM proxy cost tracking data")
    parser.add_argument(
        "cost_file",
        nargs="?",
        default="logs/cost-tracking.jsonl",
        help="Path to cost tracking JSONL file (default: logs/cost-tracking.jsonl)",
    )
    parser.add_argument(
        "--since",
        type=int,
        default=24,
        help="Analyze records from last N hours (default: 24)",
    )
    parser.add_argument(
        "--format",
        choices=["summary", "detailed", "csv", "hourly"],
        default="summary",
        help="Report format (default: summary)",
    )
    parser.add_argument(
        "--csv-output", type=str, help="CSV output filename (for csv format)"
    )

    args = parser.parse_args()

    # Load records
    records = load_cost_records(args.cost_file, args.since)

    if not records:
        return 1

    # Generate appropriate report
    if args.format == "summary":
        generate_summary_report(records)
    elif args.format == "detailed":
        generate_detailed_report(records)
    elif args.format == "csv":
        generate_csv_report(records, args.csv_output)
    elif args.format == "hourly":
        generate_hourly_breakdown(records)

    return 0


if __name__ == "__main__":
    sys.exit(main())
