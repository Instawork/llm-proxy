# LLM Proxy Configuration
# This file configures rate limiting, cost tracking, and model-specific settings

# Development configuration

enabled: true

features:
  cost_tracking:
    enabled: true
    # Enable async processing for better performance in development
    async: true
    workers: 3 # Use fewer workers in dev environment
    queue_size: 500 # Smaller queue size for development
    flush_interval: 10 # Shorter flush interval for development (10 seconds)
    transports:
      - type: "file"
        file:
          path: "./logs/cost-tracking.jsonl"
      - type: "datadog"
        datadog:
          host: "datadog-agent"
          port: "8125"
          namespace: "llm"
          tags: ["env:dev", "service:llm-proxy"]
          sample_rate: 1.0
  api_key_management:
    enabled: true
    table_name: "llm-proxy-api-keys-dev"
    region: "us-west-2"
  rate_limiting:
    enabled: true
    backend: "memory"
    estimation:
      max_sample_bytes: 200000
      bytes_per_token: 4
      chars_per_token: 4
      provider_chars_per_token:
        openai: 5 # ~185–190 tokens per 1k chars (from scripts/token_estimation.py)
        anthropic: 3 # ~290–315 tokens per 1k chars (from scripts/token_estimation.py)
    limits:
      # Global limits: 0 means unlimited in dev by default
      requests_per_minute: 0
      tokens_per_minute: 0
      requests_per_day: 0
      tokens_per_day: 0
    overrides:
    # Example Redis configuration (switch backend to "redis" to use)
    redis:
      address: "localhost:6379"
      password: ""
      db: 0
      per_user:
        # Example: apply a dev-only limit to a specific user id
        example-user:
          requests_per_minute: 5
          tokens_per_minute: 2000
          requests_per_day: 100
          tokens_per_day: 200000
